
// This file is automatically generated. Do not edit it directly.
import { createClient } from '@supabase/supabase-js';
import type { Database } from './types';

const SUPABASE_URL = "https://fhyhyfoqzpkzkxbkqcdp.supabase.co";
const SUPABASE_PUBLISHABLE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZoeWh5Zm9xenBremt4YmtxY2RwIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NjY0ODIsImV4cCI6MjA2MjE0MjQ4Mn0.NluvL4FiDbgKXu_avMaLUgyzQayV4_15vrH64vWfok0";

// Import the supabase client like this:
// import { supabase } from "@/integrations/supabase/client";

// Define types for our RPC functions with simplified typing to avoid excessive depth
interface RPCResponse<T = any> {
  data: T;
  error: Error | null;
}

// Extend SupabaseClient with simplified typing
declare module '@supabase/supabase-js' {
  interface SupabaseClient<Database> {
    rpc(
      fn: 'admin_login' | 'create_admin' | 'delete_admin' | 'add_initial_admins',
      params?: object,
      options?: object
    ): RPCResponse;
  }
}

export const supabase = createClient<Database>(SUPABASE_URL, SUPABASE_PUBLISHABLE_KEY, {
  auth: {
    persistSession: true,
    autoRefreshToken: true,
    detectSessionInUrl: true,
    storage: typeof window !== 'undefined' ? localStorage : undefined,
    flowType: 'implicit',
  },
  global: {
    headers: {
      'Cache-Control': 'no-cache',
      'Pragma': 'no-cache',
      'X-Client-Info': 'supabase-js/2.x'
    },
    fetch: (url, options) => {
      // Extended timeout to 30 minutes for complete database operations
      const timeout = 1800000; 
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), timeout);
      
      return fetch(url, {
        ...options,
        signal: controller.signal,
      }).finally(() => clearTimeout(timeoutId));
    }
  },
  db: {
    schema: 'public',
  },
  realtime: {
    params: {
      eventsPerSecond: 200, // Higher rate limit for complete database monitoring
    }
  }
});

// Enhanced connection pooling and request monitoring for complete database operations
let failedRequests = 0;
const maxRetries = 15; // Increased retries for complete database fetching
const connectionPool = new Set();
const maxPoolSize = 500; // Higher pool size for complete operations

// Optimized handleRequestWithRetry function for complete database operations
const handleRequestWithRetry = async (requestFn) => {
  let retries = 0;
  
  while (retries < maxRetries) {
    try {
      if (connectionPool.size >= maxPoolSize) {
        const delay = 25 * Math.pow(1.5, retries); // Gentler exponential backoff
        await new Promise(resolve => setTimeout(resolve, delay));
      }
      
      const requestId = Date.now() + Math.random();
      connectionPool.add(requestId);
      
      const result = await requestFn();
      
      connectionPool.delete(requestId);
      return result;
    } catch (error) {
      retries++;
      console.error(`Supabase request failed (attempt ${retries}/${maxRetries}):`, error);
      
      if (retries >= maxRetries) {
        console.error('Maximum retries reached for Supabase request');
        throw error;
      }
      
      const baseDelay = 100 * Math.pow(1.8, retries);
      const jitter = Math.random() * 200;
      await new Promise(resolve => setTimeout(resolve, baseDelay + jitter));
    }
  }
};

// Monitor for authentication failures
supabase.auth.onAuthStateChange((event, session) => {
  if (event === 'SIGNED_OUT') {
    console.log('User signed out');
    failedRequests = 0;
  } else if (event === 'USER_UPDATED') {
    console.log('User session updated');
  }
});

// Enhance original auth methods with retry logic
const originalSignIn = supabase.auth.signInWithPassword;
supabase.auth.signInWithPassword = async (credentials) => {
  return handleRequestWithRetry(async () => {
    try {
      const response = await originalSignIn(credentials);
      if (response.error) {
        failedRequests++;
        if (failedRequests > 5) {
          console.error('Multiple failed login attempts detected');
          setTimeout(() => { failedRequests = 0; }, 15 * 60 * 1000);
        }
      } else {
        failedRequests = 0;
      }
      return response;
    } catch (error) {
      console.error('Supabase auth error:', error);
      throw error;
    }
  });
};

// Optimized batch operations for complete database handling
export const batchOperation = async (items, operationFn, batchSize = 1000) => {
  const batches = [];
  
  for (let i = 0; i < items.length; i += batchSize) {
    batches.push(items.slice(i, i + batchSize));
  }
  
  const results = [];
  for (const batch of batches) {
    try {
      const result = await operationFn(batch);
      results.push(result);
    } catch (error) {
      console.error('Batch operation failed:', error);
      throw error;
    }
  }
  
  return results;
};

// ULTIMATE fetchAllRecords function with NO limitations and maximum optimization
export const fetchAllRecords = async <T>(
  tableName: 'admins' | 'voters',
  options: {
    filters?: Record<string, any>;
    orderBy?: string;
    ascending?: boolean;
  } = {}
): Promise<T[]> => {
  const { filters = {}, orderBy = 'created_at', ascending = true } = options;
  const allResults: T[] = [];
  
  console.log(`ðŸš€ ULTIMATE FETCH: Starting unlimited fetch for ${tableName}`);

  try {
    // Get total count with multiple strategies
    let expectedCount = 0;
    try {
      const { count, error: countError } = await supabase
        .from(tableName)
        .select('*', { count: 'exact', head: true });
      
      if (!countError && count !== null) {
        expectedCount = count;
        console.log(`ðŸ“Š Expected records: ${expectedCount}`);
      }
    } catch (e) {
      console.warn("Count failed, proceeding without expected count");
    }
    
    // Strategy 1: Try to get ALL records in a single massive query
    try {
      console.log("ðŸŽ¯ Attempting single massive query...");
      
      let query = supabase.from(tableName).select('*');
      
      // Add ordering
      query = query.order(orderBy, { ascending });
      
      // Apply filters
      Object.entries(filters).forEach(([key, value]) => {
        if (value !== undefined && value !== null && value !== '') {
          query = query.eq(key, value);
        }
      });

      const { data: allData, error: allError } = await query;
      
      if (!allError && allData && allData.length > 0) {
        console.log(`âœ… Single query SUCCESS: Got ${allData.length} records`);
        
        if (expectedCount > 0 && allData.length === expectedCount) {
          console.log(`ðŸŽ¯ PERFECT MATCH: ${allData.length}/${expectedCount} records`);
          return allData as T[];
        } else if (expectedCount === 0 || allData.length >= expectedCount * 0.95) {
          console.log(`âœ… ACCEPTABLE: ${allData.length} records (${expectedCount ? `vs ${expectedCount} expected` : 'no count available'})`);
          return allData as T[];
        }
        
        // Store results but continue with verification
        allResults.push(...allData as T[]);
      } else if (allError) {
        console.warn("Single query failed:", allError);
      }
    } catch (singleQueryError) {
      console.warn("Single massive query failed:", singleQueryError);
    }
    
    // Strategy 2: If single query didn't get everything, use ultra-batching
    if (allResults.length === 0 || (expectedCount > 0 && allResults.length < expectedCount)) {
      console.log("ðŸ”„ Falling back to ultra-batching strategy...");
      
      const MEGA_BATCH_SIZE = 5000; // Very large batches
      const MAX_ITERATIONS = 1000; // Safety limit
      let offset = 0;
      let iteration = 0;
      let consecutiveEmpty = 0;
      
      // Clear previous results if we're doing a complete refetch
      if (allResults.length > 0 && expectedCount > 0 && allResults.length < expectedCount) {
        allResults.length = 0;
      }
      
      while (iteration < MAX_ITERATIONS && consecutiveEmpty < 3) {
        try {
          console.log(`ðŸ“¦ Batch ${iteration + 1}: offset ${offset}, size ${MEGA_BATCH_SIZE}`);
          
          let batchQuery = supabase.from(tableName).select('*');
          
          // Add ordering
          batchQuery = batchQuery.order(orderBy, { ascending });
          
          // Add range
          batchQuery = batchQuery.range(offset, offset + MEGA_BATCH_SIZE - 1);
          
          // Apply filters
          Object.entries(filters).forEach(([key, value]) => {
            if (value !== undefined && value !== null && value !== '') {
              batchQuery = batchQuery.eq(key, value);
            }
          });

          const { data: batchData, error: batchError } = await batchQuery;
          
          if (batchError) {
            console.error(`Batch ${iteration + 1} error:`, batchError);
            break;
          }
          
          if (batchData && batchData.length > 0) {
            allResults.push(...batchData as T[]);
            consecutiveEmpty = 0;
            console.log(`âœ… Batch ${iteration + 1}: +${batchData.length} records (total: ${allResults.length})`);
            
            // If we got less than expected, we might be done
            if (batchData.length < MEGA_BATCH_SIZE) {
              console.log("ðŸ“‹ Received partial batch, likely at end");
              break;
            }
            
            offset += batchData.length;
          } else {
            consecutiveEmpty++;
            console.log(`âŒ Empty batch ${iteration + 1} (${consecutiveEmpty}/3)`);
            offset += MEGA_BATCH_SIZE; // Try skipping ahead
          }
          
          iteration++;
          
          // Small delay to prevent overwhelming the server
          if (iteration % 10 === 0) {
            await new Promise(resolve => setTimeout(resolve, 50));
          }
          
        } catch (batchError) {
          console.error(`Batch ${iteration + 1} exception:`, batchError);
          break;
        }
      }
    }
    
    // Strategy 3: Final deduplication and verification
    console.log("ðŸ” Final verification and deduplication...");
    
    // Remove duplicates based on ID
    const uniqueResults = allResults.reduce((acc, record: any) => {
      if (!acc.find((r: any) => r.id === record.id)) {
        acc.push(record);
      }
      return acc;
    }, [] as T[]);
    
    const finalCount = uniqueResults.length;
    console.log(`ðŸ FINAL RESULT: ${finalCount} unique records`);
    
    if (expectedCount > 0) {
      const percentage = ((finalCount / expectedCount) * 100).toFixed(1);
      console.log(`ðŸ“ˆ Completeness: ${percentage}% (${finalCount}/${expectedCount})`);
      
      if (finalCount < expectedCount) {
        console.warn(`âš ï¸ Missing ${expectedCount - finalCount} records`);
      }
    }
    
    return uniqueResults;
    
  } catch (error) {
    console.error('âŒ CRITICAL ERROR in fetchAllRecords:', error);
    throw error;
  }
};

// Legacy compatibility
export const fetchPaginated = fetchAllRecords;
