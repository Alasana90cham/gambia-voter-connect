
// This file is automatically generated. Do not edit it directly.
import { createClient } from '@supabase/supabase-js';
import type { Database } from './types';

const SUPABASE_URL = "https://fhyhyfoqzpkzkxbkqcdp.supabase.co";
const SUPABASE_PUBLISHABLE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZoeWh5Zm9xenBremt4YmtxY2RwIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NjY0ODIsImV4cCI6MjA2MjE0MjQ4Mn0.NluvL4FiDbgKXu_avMaLUgyzQayV4_15vrH64vWfok0";

// Import the supabase client like this:
// import { supabase } from "@/integrations/supabase/client";

// Define types for our RPC functions with simplified typing to avoid excessive depth
interface RPCResponse<T = any> {
  data: T;
  error: Error | null;
}

// Extend SupabaseClient with simplified typing
declare module '@supabase/supabase-js' {
  interface SupabaseClient<Database> {
    rpc(
      fn: 'admin_login' | 'create_admin' | 'delete_admin' | 'add_initial_admins',
      params?: object,
      options?: object
    ): RPCResponse;
  }
}

export const supabase = createClient<Database>(SUPABASE_URL, SUPABASE_PUBLISHABLE_KEY, {
  auth: {
    persistSession: true,
    autoRefreshToken: true,
    detectSessionInUrl: true,
    storage: typeof window !== 'undefined' ? localStorage : undefined,
    flowType: 'implicit', // More secure auth flow
  },
  global: {
    headers: {
      'Cache-Control': 'no-cache',
      'Pragma': 'no-cache',
      'X-Client-Info': 'supabase-js/2.x'
    },
    fetch: (url, options) => {
      // Increased timeout to 15 minutes for very large dataset operations
      const timeout = 900000; 
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), timeout);
      
      return fetch(url, {
        ...options,
        signal: controller.signal,
      }).finally(() => clearTimeout(timeoutId));
    }
  },
  db: {
    schema: 'public',
  },
  realtime: {
    params: {
      eventsPerSecond: 60, // Increased rate limit for more frequent updates
    }
  }
});

// Enhanced connection pooling and request monitoring
let failedRequests = 0;
const maxRetries = 7; // Increased for better resilience
const connectionPool = new Set();
const maxPoolSize = 200; // Increased for better concurrency

// Simplified handleRequestWithRetry function to avoid excessive type instantiation
const handleRequestWithRetry = async (requestFn) => {
  let retries = 0;
  
  while (retries < maxRetries) {
    try {
      if (connectionPool.size >= maxPoolSize) {
        const delay = 50 * Math.pow(2, retries);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
      
      const requestId = Date.now() + Math.random();
      connectionPool.add(requestId);
      
      const result = await requestFn();
      
      connectionPool.delete(requestId);
      return result;
    } catch (error) {
      retries++;
      console.error(`Supabase request failed (attempt ${retries}/${maxRetries}):`, error);
      
      if (retries >= maxRetries) {
        console.error('Maximum retries reached for Supabase request');
        throw error;
      }
      
      const baseDelay = 200 * Math.pow(2, retries);
      const jitter = Math.random() * 300;
      await new Promise(resolve => setTimeout(resolve, baseDelay + jitter));
    }
  }
};

// Monitor for authentication failures
supabase.auth.onAuthStateChange((event, session) => {
  if (event === 'SIGNED_OUT') {
    console.log('User signed out');
    failedRequests = 0; // Reset counter on sign out
  } else if (event === 'USER_UPDATED') {
    console.log('User session updated');
  }
});

// Enhance original auth methods with retry logic
const originalSignIn = supabase.auth.signInWithPassword;
supabase.auth.signInWithPassword = async (credentials) => {
  return handleRequestWithRetry(async () => {
    try {
      const response = await originalSignIn(credentials);
      if (response.error) {
        failedRequests++;
        if (failedRequests > 5) {
          console.error('Multiple failed login attempts detected');
          setTimeout(() => { failedRequests = 0; }, 15 * 60 * 1000);
        }
      } else {
        failedRequests = 0;
      }
      return response;
    } catch (error) {
      console.error('Supabase auth error:', error);
      throw error;
    }
  });
};

// Add optimized batch operations for handling large datasets
export const batchOperation = async (items, operationFn, batchSize = 750) => {
  const batches = [];
  
  for (let i = 0; i < items.length; i += batchSize) {
    batches.push(items.slice(i, i + batchSize));
  }
  
  const results = [];
  for (const batch of batches) {
    try {
      const result = await operationFn(batch);
      results.push(result);
    } catch (error) {
      console.error('Batch operation failed:', error);
      throw error;
    }
  }
  
  return results;
};

// IMPROVED: Complete reimplementation of fetchPaginated with UNLIMITED record support
// and multiple fail-safe mechanisms to ensure ALL records are retrieved
export const fetchPaginated = async <T>(
  tableName: 'admins' | 'voters',
  options: {
    filters?: Record<string, any>;
    orderBy?: string;
    ascending?: boolean;
  } = {}, 
  pageSize = 100000  // Use a MASSIVE page size to reduce requests
): Promise<T[]> => {
  const { filters = {}, orderBy = 'created_at', ascending = true } = options;
  const allResults: T[] = [];
  
  console.log(`Starting ULTRA-RELIABLE paginated fetch for ${tableName} with NO record limits`);

  try {
    // APPROACH 1: Try direct count first to verify total records
    const { count, error: countError } = await supabase
      .from(tableName)
      .select('*', { count: 'exact', head: true });
    
    if (countError) {
      console.error(`Error getting count for ${tableName}:`, countError);
    } else {
      console.log(`Total records in ${tableName}: ${count}`);
    }
    
    // APPROACH 2: Use multiple fetch strategies for maximum reliability
    // Strategy 1: Standard pagination using range
    let page = 0;
    let hasMore = true;
    const maxPages = 10000; // Virtually unlimited
    
    // Force minimum delay between requests to prevent rate limits
    const delayBetweenRequests = async () => {
      await new Promise(resolve => setTimeout(resolve, 100));
    };

    while (hasMore && page < maxPages) {
      const start = page * pageSize;
      
      // Create a fresh query for each page
      let query = supabase.from(tableName).select('*', { count: 'exact' });
      
      // Add ordering
      query = query.order(orderBy, { ascending });
      
      // Add range for current page
      query = query.range(start, start + pageSize - 1);
      
      // Apply filters
      Object.entries(filters).forEach(([key, value]) => {
        if (value !== undefined && value !== null && value !== '') {
          // @ts-ignore
          query = query.eq(key, value);
        }
      });

      console.log(`Fetching ${tableName} page ${page + 1} (${start}-${start + pageSize - 1})`);
      
      // Multiple retry mechanism for each page
      let pageRetries = 0;
      let pageData = null;
      let pageError = null;
      let pageCount = null;
      
      while (pageRetries < 3 && !pageData) {
        try {
          const { data, error, count } = await query;
          
          if (error) {
            console.error(`Error fetching page ${page + 1} (attempt ${pageRetries + 1}/3):`, error);
            pageError = error;
            pageRetries++;
            await new Promise(resolve => setTimeout(resolve, 1000 * pageRetries));
          } else {
            pageData = data;
            pageCount = count;
            break;
          }
        } catch (e) {
          console.error(`Exception fetching page ${page + 1} (attempt ${pageRetries + 1}/3):`, e);
          pageError = e;
          pageRetries++;
          await new Promise(resolve => setTimeout(resolve, 1000 * pageRetries));
        }
      }
      
      if (pageData && pageData.length > 0) {
        console.log(`Received ${pageData.length} records for page ${page + 1}`);
        allResults.push(...pageData as T[]);
        
        // Check if we've reached the total count
        if (typeof pageCount === 'number') {
          const progressPct = Math.round((allResults.length / pageCount) * 100);
          console.log(`Progress: ${allResults.length}/${pageCount} total records (${progressPct}%)`);
          
          if (allResults.length >= pageCount) {
            console.log(`Fetched all ${pageCount} records in ${page + 1} pages`);
            hasMore = false;
            break;
          }
        }
        
        // Add delay between requests
        await delayBetweenRequests();
      } else {
        console.log(`No data received for page ${page + 1}, stopping pagination`);
        hasMore = false;
      }
      
      // Continue only if we received a full page of results
      hasMore = pageData && pageData.length === pageSize;
      page++;
    }
    
    // RECOVERY MECHANISM: If we still didn't get all records, try direct fetch
    if (count && allResults.length < count) {
      console.warn(`Pagination didn't retrieve all records (${allResults.length}/${count}). Using backup method...`);
      
      // Try batched direct fetch for remaining records
      const missingIds = new Set();
      
      // First pass - collect all IDs we already have
      const existingIds = new Set(allResults.map(item => (item as any).id));
      
      // Try to fetch ALL records without pagination to fill in gaps
      try {
        let directQuery = supabase.from(tableName).select('*');
        
        // Add filters if any
        Object.entries(filters).forEach(([key, value]) => {
          if (value !== undefined && value !== null && value !== '') {
            // @ts-ignore
            directQuery = directQuery.eq(key, value);
          }
        });
        
        const { data: allData, error: directError } = await directQuery;
        
        if (directError) {
          console.error('Error in direct fetch fallback:', directError);
        } else if (allData) {
          console.log(`Direct fetch returned ${allData.length} records`);
          
          // Find records not already in our results
          const missingRecords = allData.filter(record => !existingIds.has(record.id));
          
          if (missingRecords.length > 0) {
            console.log(`Found ${missingRecords.length} missing records via direct fetch`);
            allResults.push(...missingRecords as T[]);
          }
        }
      } catch (e) {
        console.error('Exception in direct fetch fallback:', e);
      }
    }
    
    console.log(`FINAL RESULT: Fetched ${allResults.length} total records from ${tableName}`);
    return allResults;
  } catch (error) {
    console.error('Critical error in fetchPaginated:', error);
    throw error;
  }
};
