// This file is automatically generated. Do not edit it directly.
import { createClient } from '@supabase/supabase-js';
import type { Database } from './types';

const SUPABASE_URL = "https://fhyhyfoqzpkzkxbkqcdp.supabase.co";
const SUPABASE_PUBLISHABLE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZoeWh5Zm9xenBremt4YmtxY2RwIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NjY0ODIsImV4cCI6MjA2MjE0MjQ4Mn0.NluvL4FiDbgKXu_avMaLUgyzQayV4_15vrH64vWfok0";

// Import the supabase client like this:
// import { supabase } from "@/integrations/supabase/client";

// Define types for our RPC functions with simplified typing to avoid excessive depth
interface RPCResponse<T = any> {
  data: T;
  error: Error | null;
}

// Extend SupabaseClient with simplified typing
declare module '@supabase/supabase-js' {
  interface SupabaseClient<Database> {
    rpc(
      fn: 'admin_login' | 'create_admin' | 'delete_admin' | 'add_initial_admins',
      params?: object,
      options?: object
    ): RPCResponse;
  }
}

// Create the Supabase client with improved reliability settings
export const supabase = createClient<Database>(SUPABASE_URL, SUPABASE_PUBLISHABLE_KEY, {
  auth: {
    persistSession: true,
    autoRefreshToken: true,
    detectSessionInUrl: true,
    storage: typeof window !== 'undefined' ? localStorage : undefined,
    flowType: 'implicit', // More secure auth flow
  },
  global: {
    headers: {
      'Cache-Control': 'no-cache',
      'Pragma': 'no-cache',
      'X-Client-Info': 'supabase-js/2.x'
    },
    fetch: (url, options) => {
      // Increased timeout to 20 minutes for very large dataset operations
      const timeout = 1200000; 
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), timeout);
      
      return fetch(url, {
        ...options,
        signal: controller.signal,
        keepalive: true, // Ensure connection stays alive
      }).finally(() => clearTimeout(timeoutId));
    }
  },
  db: {
    schema: 'public',
  },
  realtime: {
    params: {
      eventsPerSecond: 100, // Increased rate limit for more frequent updates
    }
  }
});

// CRITICAL DATA INTEGRITY: Enhanced connection pooling and request monitoring
let failedRequests = 0;
let recoveryInProgress = false;
const maxRetries = 10; // Increased for better resilience
const connectionPool = new Set();
const maxPoolSize = 500; // Increased for better concurrency
const offlineQueue: Array<() => Promise<any>> = [];
let networkStatus: 'online' | 'offline' = 'online';

// Network status monitor
if (typeof window !== 'undefined') {
  window.addEventListener('online', () => {
    console.log('Network connection restored');
    networkStatus = 'online';
    processOfflineQueue();
  });
  
  window.addEventListener('offline', () => {
    console.log('Network connection lost, queuing operations');
    networkStatus = 'offline';
  });
}

// Process queued operations when connection is restored
const processOfflineQueue = async () => {
  if (recoveryInProgress || offlineQueue.length === 0) return;
  
  recoveryInProgress = true;
  console.log(`Processing ${offlineQueue.length} queued operations`);
  
  let successCount = 0;
  
  while (offlineQueue.length > 0) {
    const operation = offlineQueue.shift();
    if (!operation) continue;
    
    try {
      await operation();
      successCount++;
    } catch (error) {
      console.error('Failed to process queued operation:', error);
      // Re-queue critical operations with backoff
      if (offlineQueue.length < 1000) { // Prevent queue from growing infinitely
        offlineQueue.push(operation);
      }
    }
    
    // Small delay between operations to avoid overwhelming the server
    await new Promise(resolve => setTimeout(resolve, 50));
  }
  
  console.log(`Recovery completed: ${successCount} operations processed`);
  recoveryInProgress = false;
};

// SUPER-RELIABLE request handling with retry logic, queuing and persistence
export const handleRequestWithRetry = async <T>(requestFn: () => Promise<T>): Promise<T> => {
  let retries = 0;
  
  // If offline, queue the operation and return a promise that resolves when online
  if (networkStatus === 'offline') {
    return new Promise((resolve, reject) => {
      offlineQueue.push(async () => {
        try {
          const result = await handleRequestWithRetry(requestFn);
          resolve(result);
        } catch (error) {
          reject(error);
        }
      });
    });
  }
  
  while (retries < maxRetries) {
    try {
      // Connection pool management
      if (connectionPool.size >= maxPoolSize) {
        const delay = 50 * Math.pow(2, retries);
        await new Promise(resolve => setTimeout(resolve, delay + Math.random() * 100));
      }
      
      const requestId = Date.now() + Math.random();
      connectionPool.add(requestId);
      
      const result = await requestFn();
      
      connectionPool.delete(requestId);
      failedRequests = 0; // Reset counter on success
      return result;
    } catch (error: any) {
      retries++;
      console.error(`Supabase request failed (attempt ${retries}/${maxRetries}):`, error);
      
      // Add special handling for specific error types
      if (error?.message?.includes('network') || error?.message?.includes('timeout')) {
        console.warn('Network-related error detected, implementing recovery strategy');
        // Increased delay for network issues
        const baseDelay = 500 * Math.pow(2, retries);
        await new Promise(resolve => setTimeout(resolve, baseDelay + Math.random() * 500));
      } else {
        const baseDelay = 200 * Math.pow(2, retries);
        const jitter = Math.random() * 300;
        await new Promise(resolve => setTimeout(resolve, baseDelay + jitter));
      }
      
      if (retries >= maxRetries) {
        failedRequests++;
        console.error(`Maximum retries (${maxRetries}) reached for Supabase request`);
        throw new Error(`Request failed after ${maxRetries} attempts: ${error.message || 'Unknown error'}`);
      }
    }
  }
  
  throw new Error("Request failed with unknown error");
};

// Monitor for authentication failures
supabase.auth.onAuthStateChange((event, session) => {
  if (event === 'SIGNED_OUT') {
    console.log('User signed out');
    failedRequests = 0; // Reset counter on sign out
  } else if (event === 'USER_UPDATED') {
    console.log('User session updated');
  }
});

// Enhance original auth methods with retry logic
const originalSignIn = supabase.auth.signInWithPassword;
supabase.auth.signInWithPassword = async (credentials) => {
  return handleRequestWithRetry(async () => {
    try {
      const response = await originalSignIn(credentials);
      if (response.error) {
        failedRequests++;
        if (failedRequests > 5) {
          console.error('Multiple failed login attempts detected');
          setTimeout(() => { failedRequests = 0; }, 15 * 60 * 1000);
        }
      } else {
        failedRequests = 0;
      }
      return response;
    } catch (error) {
      console.error('Supabase auth error:', error);
      throw error;
    }
  });
};

// Add optimized batch operations for handling large datasets
export const batchOperation = async (items, operationFn, batchSize = 750) => {
  const batches = [];
  
  for (let i = 0; i < items.length; i += batchSize) {
    batches.push(items.slice(i, i + batchSize));
  }
  
  const results = [];
  for (const batch of batches) {
    try {
      const result = await operationFn(batch);
      results.push(result);
    } catch (error) {
      console.error('Batch operation failed:', error);
      throw error;
    }
  }
  
  return results;
};

// CRITICAL: Implement failsafe data insurance for form submissions
export const insuranceSubmit = async <T>(
  tableName: string,
  data: any,
  attempts = 3
): Promise<T> => {
  // First, save to local storage as backup
  const localBackupKey = `${tableName}_backup_${Date.now()}`;
  if (typeof localStorage !== 'undefined') {
    try {
      localStorage.setItem(localBackupKey, JSON.stringify({
        table: tableName,
        data: data,
        timestamp: Date.now()
      }));
    } catch (e) {
      console.warn('Could not save backup to localStorage', e);
      // Continue anyway - localStorage is just a backup
    }
  }
  
  // Try to submit to Supabase
  try {
    const result = await handleRequestWithRetry(async () => {
      const { data: responseData, error } = await supabase
        .from(tableName)
        .insert(data)
        .select();
        
      if (error) throw error;
      return responseData;
    });
    
    // On success, clean up the local backup
    if (typeof localStorage !== 'undefined') {
      try {
        localStorage.removeItem(localBackupKey);
      } catch (e) {
        // Ignore cleanup errors
      }
    }
    
    return result as T;
  } catch (error) {
    console.error(`Failed to submit data to ${tableName} after multiple attempts`, error);
    
    // Failed after all attempts, add to recovery queue
    offlineQueue.push(async () => {
      return insuranceSubmit(tableName, data, attempts + 1);
    });
    
    throw error;
  }
};

// ENHANCED: Complete reimplementation of fetchPaginated with UNLIMITED record support
// and multiple fail-safe mechanisms to ensure ALL records are retrieved
export const fetchPaginated = async <T>(
  tableName: 'admins' | 'voters',
  options: {
    filters?: Record<string, any>;
    orderBy?: string;
    ascending?: boolean;
  } = {}, 
  pageSize = 100000  // Use a MASSIVE page size to reduce requests
): Promise<T[]> => {
  const { filters = {}, orderBy = 'created_at', ascending = true } = options;
  const allResults: T[] = [];
  
  console.log(`Starting ULTRA-RELIABLE paginated fetch for ${tableName} with NO record limits`);

  try {
    // APPROACH 1: Try direct count first to verify total records
    const { count, error: countError } = await supabase
      .from(tableName)
      .select('*', { count: 'exact', head: true });
    
    if (countError) {
      console.error(`Error getting count for ${tableName}:`, countError);
    } else {
      console.log(`Total records in ${tableName}: ${count}`);
    }
    
    // APPROACH 2: Use multiple fetch strategies for maximum reliability
    // Strategy 1: Standard pagination using range
    let page = 0;
    let hasMore = true;
    const maxPages = 10000; // Virtually unlimited
    
    // Force minimum delay between requests to prevent rate limits
    const delayBetweenRequests = async () => {
      await new Promise(resolve => setTimeout(resolve, 100));
    };

    while (hasMore && page < maxPages) {
      const start = page * pageSize;
      
      // Create a fresh query for each page
      let query = supabase.from(tableName).select('*', { count: 'exact' });
      
      // Add ordering
      query = query.order(orderBy, { ascending });
      
      // Add range for current page
      query = query.range(start, start + pageSize - 1);
      
      // Apply filters
      Object.entries(filters).forEach(([key, value]) => {
        if (value !== undefined && value !== null && value !== '') {
          // @ts-ignore
          query = query.eq(key, value);
        }
      });

      console.log(`Fetching ${tableName} page ${page + 1} (${start}-${start + pageSize - 1})`);
      
      // Multiple retry mechanism for each page
      let pageRetries = 0;
      let pageData = null;
      let pageError = null;
      let pageCount = null;
      
      while (pageRetries < 3 && !pageData) {
        try {
          const { data, error, count } = await query;
          
          if (error) {
            console.error(`Error fetching page ${page + 1} (attempt ${pageRetries + 1}/3):`, error);
            pageError = error;
            pageRetries++;
            await new Promise(resolve => setTimeout(resolve, 1000 * pageRetries));
          } else {
            pageData = data;
            pageCount = count;
            break;
          }
        } catch (e) {
          console.error(`Exception fetching page ${page + 1} (attempt ${pageRetries + 1}/3):`, e);
          pageError = e;
          pageRetries++;
          await new Promise(resolve => setTimeout(resolve, 1000 * pageRetries));
        }
      }
      
      if (pageData && pageData.length > 0) {
        console.log(`Received ${pageData.length} records for page ${page + 1}`);
        allResults.push(...pageData as T[]);
        
        // Check if we've reached the total count
        if (typeof pageCount === 'number') {
          const progressPct = Math.round((allResults.length / pageCount) * 100);
          console.log(`Progress: ${allResults.length}/${pageCount} total records (${progressPct}%)`);
          
          if (allResults.length >= pageCount) {
            console.log(`Fetched all ${pageCount} records in ${page + 1} pages`);
            hasMore = false;
            break;
          }
        }
        
        // Add delay between requests
        await delayBetweenRequests();
      } else {
        console.log(`No data received for page ${page + 1}, stopping pagination`);
        hasMore = false;
      }
      
      // Continue only if we received a full page of results
      hasMore = pageData && pageData.length === pageSize;
      page++;
    }
    
    // RECOVERY MECHANISM: If we still didn't get all records, try direct fetch
    if (count && allResults.length < count) {
      console.warn(`Pagination didn't retrieve all records (${allResults.length}/${count}). Using backup method...`);
      
      // Try batched direct fetch for remaining records
      const missingIds = new Set();
      
      // First pass - collect all IDs we already have
      const existingIds = new Set(allResults.map(item => (item as any).id));
      
      // Try to fetch ALL records without pagination to fill in gaps
      try {
        let directQuery = supabase.from(tableName).select('*');
        
        // Add filters if any
        Object.entries(filters).forEach(([key, value]) => {
          if (value !== undefined && value !== null && value !== '') {
            // @ts-ignore
            directQuery = directQuery.eq(key, value);
          }
        });
        
        const { data: allData, error: directError } = await directQuery;
        
        if (directError) {
          console.error('Error in direct fetch fallback:', directError);
        } else if (allData) {
          console.log(`Direct fetch returned ${allData.length} records`);
          
          // Find records not already in our results
          const missingRecords = allData.filter(record => !existingIds.has(record.id));
          
          if (missingRecords.length > 0) {
            console.log(`Found ${missingRecords.length} missing records via direct fetch`);
            allResults.push(...missingRecords as T[]);
          }
        }
      } catch (e) {
        console.error('Exception in direct fetch fallback:', e);
      }
    }
    
    // Add additional recovery mechanism to check for broken records
    try {
      // Find any orphaned/stuck records in local backups
      if (typeof localStorage !== 'undefined') {
        const potentialBackups = [];
        for (let i = 0; i < localStorage.length; i++) {
          const key = localStorage.key(i);
          if (key && key.includes(`${tableName}_backup_`)) {
            try {
              const backupData = JSON.parse(localStorage.getItem(key) || '');
              if (backupData && backupData.table === tableName) {
                potentialBackups.push({ key, data: backupData });
              }
            } catch (e) {
              // Skip invalid JSON
            }
          }
        }
        
        if (potentialBackups.length > 0) {
          console.log(`Found ${potentialBackups.length} potential ${tableName} records in local storage`);
          // We could trigger recovery here if needed
        }
      }
    } catch (e) {
      // Ignore local storage errors
    }
    
    console.log(`FINAL RESULT: Fetched ${allResults.length} total records from ${tableName}`);
    return allResults;
  } catch (error) {
    console.error('Critical error in fetchPaginated:', error);
    throw error;
  }
};

// Run recovery check on initialization
if (typeof window !== 'undefined') {
  window.addEventListener('load', () => {
    setTimeout(() => {
      processOfflineQueue();
    }, 1000);
  });
}
